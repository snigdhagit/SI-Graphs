{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from selectinf.nbd_lasso import nbd_lasso\n",
    "from selectinf.Utils.discrete_family import discrete_family\n",
    "from instance import GGM_instance\n",
    "\n",
    "from nbd_naive_and_ds import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom perturbation\n",
      "Sampled perturbation\n",
      "(10, 9)\n",
      "2.0\n",
      "2.0\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add root n to the randomization covariance\n",
    "# Remark: Not needed (?) anymore since X is now scaled\n",
    "prec,cov,X = GGM_instance(n=1000,p=10, max_edges=2)\n",
    "nbd_instance = nbd_lasso.gaussian(X, n_scaled=False)\n",
    "active_signs_nonrandom = nbd_instance.fit(perturb=np.zeros((10,9)))\n",
    "active_signs_random = nbd_instance.fit()\n",
    "print(active_signs_nonrandom.shape)\n",
    "print(np.abs(active_signs_nonrandom).sum())\n",
    "print(np.abs(active_signs_random).sum())\n",
    "print(np.abs(prec != 0).sum() - 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def data_splitting(X, prec, proportion=0.5):\n",
    "    # Precision matrix is in its original order, not scaled by root n\n",
    "    # X is also in its original order\n",
    "    n,p = X.shape\n",
    "    pi_s = proportion\n",
    "    subset_select = np.zeros(n, np.bool_)\n",
    "    subset_select[:int(pi_s * n)] = True\n",
    "    n1 = subset_select.sum()\n",
    "    n2 = n - n1\n",
    "    np.random.shuffle(subset_select)\n",
    "\n",
    "    # Rescale X_S and X_NS for numerical stability\n",
    "    X_S = X[subset_select, :] / np.sqrt(n1)\n",
    "    X_NS = X[~subset_select, :] / np.sqrt(n2)\n",
    "\n",
    "    nbd_instance = nbd_lasso.gaussian(X_S, n_scaled=True)\n",
    "    active_signs_nonrandom = nbd_instance.fit(perturb=np.zeros((p,p-1)))\n",
    "    nonzero = get_nonzero(active_signs_nonrandom)\n",
    "    # print(\"Data Splitting |E|:\", nonzero.sum())\n",
    "\n",
    "    # Construct intervals\n",
    "    if nonzero.sum() > 0:\n",
    "        # Intervals returned is in its original (unscaled) order\n",
    "        # intervals, condlDists = conditional_inference(X_NS, nonzero=nonzero)\n",
    "        intervals = conditional_inference(X_NS, nonzero=nonzero)\n",
    "        # coverage is upper-triangular\n",
    "        coverage = get_coverage(nonzero, intervals, prec * n2, n2, p)\n",
    "        interval_len = 0\n",
    "        nonzero_count = 0\n",
    "        for i in range(p):\n",
    "            for j in range(i+1,p):\n",
    "                if nonzero[i,j]:\n",
    "                    interval = intervals[i,j,:]\n",
    "                    interval_len = interval_len + (interval[1] - interval[0])\n",
    "                    nonzero_count = nonzero_count + 1\n",
    "        avg_len = interval_len / nonzero_count\n",
    "        cov_rate = coverage.sum() / nonzero_count\n",
    "        return nonzero, intervals, cov_rate, avg_len\n",
    "    return None, None, None, None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom perturbation\n",
      "143.7203195439502\n",
      "normalizer 9.326974142923583e-16\n",
      "normalizer == 0 False\n",
      "log normalizer: -34.60845084044608\n",
      "normalizer 6.258425627863547e-07\n",
      "normalizer == 0 False\n",
      "log normalizer: -14.28416699462342\n",
      "normalizer 0.09134455914379329\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.3931165584815295\n",
      "normalizer 0.10822693064096982\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2235250466728083\n",
      "normalizer 0.11600008675481782\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.1541643399896917\n",
      "normalizer 0.109998495575175\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2072885898725643\n",
      "normalizer 0.10844012582183135\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2215570940624954\n",
      "normalizer 0.10667968544903993\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2379245282193527\n",
      "normalizer 0.10704244804496267\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.23452981248853\n",
      "normalizer 0.10753126569641361\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2299736300136774\n",
      "normalizer 0.10785308958062033\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2269852595418023\n",
      "normalizer 0.10803348485210792\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225314054984988\n",
      "normalizer 0.10794165897117303\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2261643925270076\n",
      "normalizer 0.10798716447796394\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225742906347092\n",
      "normalizer 0.10796430991340315\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2259545702378856\n",
      "normalizer 0.107975711737029\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225848968473569\n",
      "normalizer 0.107981431742097\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225795994953201\n",
      "normalizer 0.10797857014830518\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258224960994033\n",
      "normalizer 0.10798000054737411\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258092491227788\n",
      "normalizer 0.10798071604527791\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225802622937099\n",
      "normalizer 0.10798107386882208\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225799309169934\n",
      "normalizer 0.10798089495083434\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258009661097065\n",
      "normalizer 0.10798080549650121\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258017945374595\n",
      "normalizer 0.10798085022327955\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258013803270926\n",
      "normalizer 0.10798082785979399\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225801587433147\n",
      "normalizer 0.1079808166781234\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258016909855223\n",
      "normalizer 0.10798082226895181\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225801639209397\n",
      "normalizer 0.10798081947353673\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258016650974675\n",
      "normalizer 0.10798082087124387\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.225801652153436\n",
      "normalizer 0.10798082157009847\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2258016456814107\n",
      "normalizer 3.5268921626820636e-14\n",
      "normalizer == 0 False\n",
      "log normalizer: -30.975774225929673\n",
      "normalizer 0.10822693064096982\n",
      "normalizer == 0 False\n",
      "log normalizer: -2.2235250466728083\n",
      "normalizer 6.258425627863547e-07\n",
      "normalizer == 0 False\n",
      "log normalizer: -14.28416699462342\n",
      "normalizer 3.139143891455978e-11\n",
      "normalizer == 0 False\n",
      "log normalizer: -24.184485906222154\n",
      "normalizer 3.0828153532520536e-09\n",
      "normalizer == 0 False\n",
      "log normalizer: -19.597422581742556\n",
      "normalizer 4.0248229003955e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -17.028199833951604\n",
      "normalizer 1.5534365408186955e-07\n",
      "normalizer == 0 False\n",
      "log normalizer: -15.677626051108943\n",
      "normalizer 7.864470454670087e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.35832553905785\n",
      "normalizer 1.1038159316943413e-07\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.0193224455419\n",
      "normalizer 9.314005890874263e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.189161466917152\n",
      "normalizer 1.0138646627753834e-07\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.104326223360392\n",
      "normalizer 9.717377906186467e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.146764924610547\n",
      "normalizer 9.925725296986016e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.125550842267195\n",
      "normalizer 9.820986182541879e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.136159200707574\n",
      "normalizer 9.769041472233607e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.14146239200102\n",
      "normalizer 9.794978586871919e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.13881087868669\n",
      "normalizer 9.807973562362385e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.137485060279843\n",
      "normalizer 9.801473870559312e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138147974628996\n",
      "normalizer 9.79822567789209e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.13847942794428\n",
      "normalizer 9.796601994699981e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138645153637096\n",
      "normalizer 9.797413801872538e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.13856229087109\n",
      "normalizer 9.79700788968077e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138603722274194\n",
      "normalizer 9.797210843625228e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.13858300657767\n",
      "normalizer 9.797109366115136e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138593364427187\n",
      "normalizer 9.797058627763485e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138598543351005\n",
      "normalizer 9.797083996905713e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138595953889173\n",
      "normalizer 9.797096681502022e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.1385946591582\n",
      "normalizer 9.797090339201765e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.13859530652369\n",
      "normalizer 9.797087168053203e-08\n",
      "normalizer == 0 False\n",
      "log normalizer: -16.138595630206435\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nonzero, intervals, cov_rate, avg_len = data_splitting(X, prec, proportion = 0.67)\n",
    "print(cov_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def edge_inference_scipy(j0, k0, S, n, p, Theta_hat=None, var=None, level=0.9, ngrid=10000):\n",
    "    # n_total: the total data points in data splitting\n",
    "    #        : the raw dimension of X in naive\n",
    "    inner_prod = S[j0,k0]\n",
    "\n",
    "    # Theta_hat: A low dimensional point estimate of theta\n",
    "    if Theta_hat is None:\n",
    "        t_j_k = - inner_prod * n\n",
    "    else:\n",
    "        t_j_k = Theta_hat[j0,k0] * n\n",
    "    print(t_j_k)\n",
    "\n",
    "    def log_det_S_j_k(s_val):\n",
    "        S_j_k = S\n",
    "        S_j_k[j0,k0] = s_val\n",
    "        S_j_k[k0,j0] = s_val\n",
    "        return np.log(np.abs(np.linalg.det(S_j_k))) * (n-p-1)/2\n",
    "    def det_S_j_k(s_val):\n",
    "        return np.exp(log_det_S_j_k(s_val))\n",
    "\n",
    "    \"\"\"def get_logdet_normalizer():\n",
    "        sparse_grid = np.linspace(-2, 2, num=100)\n",
    "        sparse_log_det = np.zeros((ngrid,))\n",
    "        for g in range(100):\n",
    "            sparse_log_det[g] = log_det_S_j_k(sparse_grid[g])\n",
    "        log_det_normalizer = np.max(sparse_log_det)\n",
    "        return log_det_normalizer\n",
    "\n",
    "    ldn = get_logdet_normalizer()\n",
    "    print(\"ldn\", np.exp(ldn))\n",
    "    # normalized determinant by multiplying 1/det_S_j_k(0)\n",
    "\n",
    "    def det_S_j_k_nomalized(s_val, log_normalizer=0):\n",
    "        return np.exp(log_det_S_j_k(s_val) - np.log(det_S_j_k(0)))\"\"\"\n",
    "\n",
    "    def condl_pdf(t,theta=0):\n",
    "        return det_S_j_k(t) * np.exp(-theta*t)\n",
    "\n",
    "    def condl_log_pdf(t,theta=0):\n",
    "        return log_det_S_j_k(t) - theta * t\n",
    "\n",
    "    def get_pivot(theta0=0):\n",
    "        # Normalize the pdf by the maximum over a sparse grid\n",
    "        def get_pdf_log_normalizer(theta0):\n",
    "            sparse_grid = np.linspace(-2, 2, num=100)\n",
    "            sparse_lpdf = np.zeros((ngrid,))\n",
    "            for g in range(100):\n",
    "                sparse_lpdf[g] = condl_log_pdf(sparse_grid[g],theta0)\n",
    "            pdf_log_normalizer = np.max(sparse_lpdf)\n",
    "\n",
    "            if pdf_log_normalizer > 100:\n",
    "                pdf_log_normalizer /= 10\n",
    "            return pdf_log_normalizer\n",
    "\n",
    "        pdfln = get_pdf_log_normalizer(theta0)\n",
    "        #print(\"pdfln\", pdfln)\n",
    "        # Normalized pdf\n",
    "        def condl_pdf_normalized(t, theta=0):\n",
    "            return det_S_j_k(t) * np.exp(-theta*t - pdfln)\n",
    "\n",
    "        grid_lb = -1\n",
    "        grid_ub = 1\n",
    "        normalizer = quad(condl_pdf_normalized,\n",
    "                          grid_lb,\n",
    "                          grid_ub, args=(theta0,))[0]\n",
    "        #print(normalizer)\n",
    "\n",
    "        cdf_upper = quad(condl_pdf_normalized, inner_prod, grid_ub,\n",
    "                         args=(theta0,))[0] / normalizer\n",
    "\n",
    "        \"\"\"stat_grid = np.zeros((1, ngrid))\n",
    "\n",
    "        stat_grid[0,:] = np.linspace(grid_lb,\n",
    "                                     grid_ub,\n",
    "                                     num=ngrid)\n",
    "        density = np.zeros((ngrid,))\n",
    "        logdet = np.zeros((ngrid,))\n",
    "        for g in range(ngrid):\n",
    "            density[g] = condl_pdf_normalized(stat_grid[0,g], theta0)\n",
    "            logdet[g] = det_S_j_k(stat_grid[0,g])\n",
    "\n",
    "        plt.plot(stat_grid[0,:], density)\n",
    "        plt.figure(2)\n",
    "        plt.plot(stat_grid[0,:], logdet)\"\"\"\n",
    "        return cdf_upper\n",
    "\n",
    "    pivot = get_pivot(0)\n",
    "\n",
    "    def get_pivot_val(x,val=0.99):\n",
    "        return get_pivot(x) - val\n",
    "\n",
    "    print(\"bracket\", t_j_k-0.2*n,t_j_k+0.2*n)\n",
    "\n",
    "    # Construct CI\n",
    "    margin = (1 - level) / 2\n",
    "    \"\"\"root_low = root_scalar(get_pivot_val, bracket=[t_j_k-0.1*n,t_j_k+0.3*n], args=(margin,),\n",
    "                           method='bisect')\n",
    "    root_up = root_scalar(get_pivot_val, bracket=[t_j_k-0.3*n,t_j_k+0.1*n], args=(1 - margin,),\n",
    "                           method='bisect')\"\"\"\n",
    "    root_low = find_root(f=get_pivot, y=margin, lb=t_j_k-0.1*n, ub=t_j_k+0.3*n, tol=1e-6)\n",
    "    root_up = find_root(f=get_pivot, y=1-margin, lb=t_j_k-0.3*n, ub=t_j_k+0.1*n, tol=1e-6)\n",
    "\n",
    "    return pivot, root_up, root_low"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom perturbation\n",
      "(i,j) = ( 5 , 9 )\n",
      "Theta 0.28070579437147164\n"
     ]
    },
    {
     "data": {
      "text/plain": "'# Construct intervals\\nif nonzero.sum() > 0:\\n    # Intervals returned is in its original (unscaled) order\\n    # intervals, condlDists = conditional_inference(X_NS, nonzero=nonzero)\\n    intervals = conditional_inference(X_NS, nonzero=nonzero)\\n    # coverage is upper-triangular\\n    coverage = get_coverage(nonzero, intervals, prec * n2, n2, p)\\n    interval_len = 0\\n    nonzero_count = 0\\n    for i in range(p):\\n        for j in range(i+1,p):\\n            if nonzero[i,j]:\\n                interval = intervals[i,j,:]\\n                interval_len = interval_len + (interval[1] - interval[0])\\n                nonzero_count = nonzero_count + 1\\n    avg_len = interval_len / nonzero_count\\n    cov_rate = coverage.sum() / nonzero_count'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision matrix is in its original order, not scaled by root n\n",
    "# X is also in its original order\n",
    "n,p = X.shape\n",
    "pi_s = proportion = 0.67\n",
    "subset_select = np.zeros(n, np.bool_)\n",
    "subset_select[:int(pi_s * n)] = True\n",
    "n1 = subset_select.sum()\n",
    "n2 = n - n1\n",
    "np.random.shuffle(subset_select)\n",
    "\n",
    "# Rescale X_S and X_NS for numerical stability\n",
    "X_S = X[subset_select, :] / np.sqrt(n1)\n",
    "X_NS = X[~subset_select, :] / np.sqrt(n2)\n",
    "\n",
    "nbd_instance = nbd_lasso.gaussian(X_S, n_scaled=True)\n",
    "active_signs_nonrandom = nbd_instance.fit(perturb=np.zeros((p,p-1)))\n",
    "nonzero = get_nonzero(active_signs_nonrandom)\n",
    "# print(\"Data Splitting |E|:\", nonzero.sum())\n",
    "\n",
    "for i in range(p):\n",
    "    for j in range(i+1,p):\n",
    "        if nonzero[i,j]:\n",
    "            print(\"(i,j) = (\",i,\",\",j,\")\")\n",
    "            print(\"Theta\", prec[i,j])\n",
    "\n",
    "\"\"\"# Construct intervals\n",
    "if nonzero.sum() > 0:\n",
    "    # Intervals returned is in its original (unscaled) order\n",
    "    # intervals, condlDists = conditional_inference(X_NS, nonzero=nonzero)\n",
    "    intervals = conditional_inference(X_NS, nonzero=nonzero)\n",
    "    # coverage is upper-triangular\n",
    "    coverage = get_coverage(nonzero, intervals, prec * n2, n2, p)\n",
    "    interval_len = 0\n",
    "    nonzero_count = 0\n",
    "    for i in range(p):\n",
    "        for j in range(i+1,p):\n",
    "            if nonzero[i,j]:\n",
    "                interval = intervals[i,j,:]\n",
    "                interval_len = interval_len + (interval[1] - interval[0])\n",
    "                nonzero_count = nonzero_count + 1\n",
    "    avg_len = interval_len / nonzero_count\n",
    "    cov_rate = coverage.sum() / nonzero_count\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-92.21428786134462\n",
      "bracket -158.21428786134462 -26.214287861344616\n"
     ]
    }
   ],
   "source": [
    "S=X_NS.T @ X_NS\n",
    "theta_hat = np.linalg.inv(S)\n",
    "pivot, lcb, ucb = edge_inference_scipy(j0=0,k0=5,S=S,n=n2,p=p,Theta_hat=theta_hat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(-0.3712540516773781, -0.17657569741407247)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcb/n2, ucb/n2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 3 , 7 ) selected\n",
      "Theta ( 3 , 7 ) interval: [0.20787315 0.38201774]\n",
      "Theta ( 3 , 7 ) 0.0\n",
      "S/n ( 3 , 7 ) -0.002432089772410599\n"
     ]
    }
   ],
   "source": [
    "def print_nonzero_intervals(nonzero, intervals, prec, X):\n",
    "    # Intervals, prec, X are all in their original scale\n",
    "    n, p = X.shape\n",
    "    S = X.T @ X / n\n",
    "\n",
    "    for i in range(p):\n",
    "            for j in range(i+1,p):\n",
    "                if nonzero[i,j]:\n",
    "                    print(\"(\",i,\",\",j,\")\", \"selected\")\n",
    "                    print(\"Theta\", \"(\",i,\",\",j,\")\", \"interval:\", intervals[i,j,:])\n",
    "                    print(\"Theta\", \"(\",i,\",\",j,\")\", prec[i,j])\n",
    "                    print(\"S/n\", \"(\",i,\",\",j,\")\", S[i,j])\n",
    "print_nonzero_intervals(nonzero, intervals, prec, X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
